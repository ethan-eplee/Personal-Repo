{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226f12e6",
   "metadata": {},
   "source": [
    "\n",
    "## Boosting as an Ensemble Method\n",
    "\n",
    "---\n",
    "Boosting takes a weak base learner and tries to make it a strong learner by retraining it on the misclassified samples.\n",
    "\n",
    "1) **Base model fitting is an iterative procedure**: It cannot be run in parallel.\n",
    "- **Weights are assigned to observations to indicate their \"importance:\"** Samples with higher weights are given higher influence on the total error of the next model, prioritizing those observations.\n",
    "- **Weights change at each iteration with the goal of correcting the errors/misclassifications of the previous iteration**: The first base estimator is fit with uniform weights on the observations.\n",
    "- **Final prediction is typically constructed by a weighted vote**: Weights for each base model depend on their training errors or misclassification rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434579c",
   "metadata": {},
   "source": [
    "## Pros and Cons of Boosting\n",
    "\n",
    "---\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Achieves higher performance than bagging when the hyperparameters are properly tuned.\n",
    "- Works equally well for classification and regression.\n",
    "- Can use \"robust\" loss functions that make the model resistant to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Difficult and time consuming to properly tune hyperparameters.\n",
    "- Cannot be parallelized like bagging (bad scalability when there are huge amounts of data).\n",
    "- Higher risk of overfitting compared to bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116a4a1",
   "metadata": {},
   "source": [
    "## Boosting and the Bias-Variance Trade-Off\n",
    "\n",
    "---\n",
    "\n",
    "Recall that **bagging aims to reduce variance** (reduce overfitting).\n",
    "\n",
    "**Boosting aims to reduce bias** (and can reduce variance a bit as well)! (reduce underfitting)\n",
    "\n",
    "### Why?\n",
    "\n",
    "The rationale/theory behind boosting is to combine **many weak learners into a single strong learner.** Instead of using deep/full decision trees like in bagging, **boosting uses shallow/high-bias base estimators.**\n",
    "\n",
    "Thus, each weak learner has:\n",
    "\n",
    "- Low variance.\n",
    "- High bias.\n",
    "\n",
    "It uses iterative fitting to explain error/misclassification unexplained by the previous base models and reduces bias without increasing variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ab1bf",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "---\n",
    "\n",
    "The core principle of AdaBoost is to **fit a sequence of weak learners** (i.e., models that are only slightly better than random guessing, such as a single-split tree) **on repeatedly modified versions of the data**. After each fit, the importance weights on each observation need to be updated. \n",
    "\n",
    "The predictions are then combined through a weighted majority vote (or sum) to produce the final prediction. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier itself, represented by alpha $\\alpha$:\n",
    "\n",
    "As iterations continue, **examples that are difficult to predict receive ever-increasing influence**. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad309b",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "---\n",
    "\n",
    "Gradient boosting classifiers are a generalization of boosting to arbitrary, differentiable loss functions. The intuition behind this mechanism is to:\n",
    "\n",
    "1. Fit a model $F$ to the data.\n",
    "2. Look at the difference between our observed $y$ and our model $F$. (The $y_i - F(x_i)$ can be thought of as residuals!)\n",
    "3. Fit a second model, $F_2$, to (roughly) the residuals $y_i - F(x_i)$.\n",
    "4. Aggregate your model $F$ and $F_2$. While we won't get into the details now, we can interpret residuals as negative gradients. By doing this, we can apply our gradient descent algorithm to optimize our loss and generalize this to many loss functions.\n",
    "\n",
    "GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient tree boosting models are used in a variety of areas, including web search ranking and ecology.\n",
    "\n",
    "**The advantages of GBRT are:**\n",
    "\n",
    "- Natural handling of mixed data types (= heterogeneous features).\n",
    "- Predictive power.\n",
    "- Robustness to outliers in output space (via robust loss functions).\n",
    "\n",
    "**The disadvantages of GBRT are:**\n",
    "- Scalability: Due to the sequential nature of boosting, it can hardly be parallelized.\n",
    "- Difficult hyperparameters to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cb644",
   "metadata": {},
   "source": [
    "## The Difference Between the AdaBoost and Gradient Boosting\n",
    "---\n",
    "- AdaBoost is about reweighting the preceding model's errors in subsequent iterations.\n",
    "- Gradient boosting is about fitting subsequent models to the residuals of the last model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e429af2",
   "metadata": {},
   "source": [
    "## Code for boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a058e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f678f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from make_classificatioabsn\n",
    "X, y = make_classification(n_samples=750, n_features=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2fcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0454e8eb",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72dc5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9216729244889446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'base_estimator__max_depth': 2, 'learning_rate': 0.9, 'n_estimators': 100}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "ada_params = {\n",
    "    'n_estimators': [50,100],\n",
    "    'base_estimator__max_depth': [1,2],\n",
    "    'learning_rate': [.9, 1.]\n",
    "}\n",
    "gs = GridSearchCV(ada, param_grid=ada_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795062e9",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243c8ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9341601243979216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier()\n",
    "gboost_params = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [.08, .1, .12]\n",
    "}\n",
    "gb_gs = GridSearchCV(gboost, param_grid=gboost_params, cv=3)\n",
    "gb_gs.fit(X_train, y_train)\n",
    "print(gb_gs.best_score_)\n",
    "gb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9ce09",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "369c5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9305855804604241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ada__n_estimators': 75, 'gb__n_estimators': 100, 'tree__max_depth': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "vote_params = {\n",
    "    'ada__n_estimators': [50,75],\n",
    "    'gb__n_estimators': [100,125],\n",
    "    'tree__max_depth': [None, 5]\n",
    "}\n",
    "gs = GridSearchCV(vote, param_grid=vote_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898c8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
