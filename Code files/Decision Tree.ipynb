{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab99f8e",
   "metadata": {},
   "source": [
    "## Build a single Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d700b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import model.\n",
    "# from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate model with random_state = 42.\n",
    "# dt = DecisionTreeClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit model.\n",
    "# dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate model.\n",
    "# print(f'Score on training set: {dt.score(X_train, y_train)}')\n",
    "# print(f'Score on testing set: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2222559",
   "metadata": {},
   "source": [
    "Decision trees tend to overfit. To solve this problem,\n",
    "- As with all models, try to gather more data.\n",
    "- As with all models, remove some features.\n",
    "- Stop our model from growing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b3d96",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters of Decision Trees\n",
    "There are four hyperparameters of decision trees that we may commonly tune in order to prevent overfitting.\n",
    "\n",
    "- `max_depth`: The maximum depth of the tree.\n",
    "    - By default, the nodes are expanded until all leaves are pure (or some other argument limits the growth of the tree).\n",
    "    - In the 20 questions analogy, this is like \"How many questions we can ask?\"\n",
    "    \n",
    "    \n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "    - By default, the minimum number of samples required to split is 2. That is, if there are two or more observations in a node and if we haven't already achieved maximum purity, we can split it!\n",
    "    \n",
    "    \n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node (a terminal node at the end of the tree).\n",
    "    - By default, the minimum number of samples required in a leaf node is 1. (This should ring alarm bells - it's very possible that we'll overfit our model to the data!)\n",
    "\n",
    "\n",
    "- `ccp_alpha`: A [complexity parameter](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning) similar to $\\alpha$ in regularization. As `ccp_alpha` increases, we regularize more.\n",
    "    - By default, this value is 0.\n",
    "\n",
    "[Source: Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate model with:\n",
    "# # - a maximum depth of 5.\n",
    "# # - at least 7 samples required in order to split an internal node.\n",
    "# # - at least 3 samples in each leaf node.\n",
    "# # - a cost complexity of 0.01.\n",
    "# # - random state of 42.\n",
    "\n",
    "# dt = DecisionTreeClassifier(max_depth = 5,\n",
    "#                             min_samples_split = 7,\n",
    "#                             min_samples_leaf = 3,\n",
    "#                             ccp_alpha = 0.01,\n",
    "#                             random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5a878",
   "metadata": {},
   "source": [
    "## Use GridSearch to tune hyperparameters and find better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e192b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b088028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(estimator = DecisionTreeClassifier(),\n",
    "#                     param_grid = {'max_depth': [2, 3, 5, 7],\n",
    "#                                   'min_samples_split': [5, 10, 15, 20],\n",
    "#                                   'min_samples_leaf': [2, 3, 4, 5, 6],\n",
    "#                                   'ccp_alpha': [0, 0.001, 0.01, 0.1, 1, 10]},\n",
    "#                     cv = 5,\n",
    "#                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b825a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # Start our timer.\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Let's GridSearch over the above parameters on our training data.\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # Stop our timer and print the result.\n",
    "# print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa042e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
