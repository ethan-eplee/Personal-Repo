{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab99f8e",
   "metadata": {},
   "source": [
    "## Build a single Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d700b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import model.\n",
    "# from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate model with random_state = 42.\n",
    "# dt = DecisionTreeClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit model.\n",
    "# dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate model.\n",
    "# print(f'Score on training set: {dt.score(X_train, y_train)}')\n",
    "# print(f'Score on testing set: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2222559",
   "metadata": {},
   "source": [
    "Decision trees tend to overfit. To solve this problem,\n",
    "- As with all models, try to gather more data.\n",
    "- As with all models, remove some features.\n",
    "- Stop our model from growing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b3d96",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters of Decision Trees\n",
    "There are four hyperparameters of decision trees that we may commonly tune in order to prevent overfitting.\n",
    "\n",
    "- `max_depth`: The maximum depth of the tree.\n",
    "    - By default, the nodes are expanded until all leaves are pure (or some other argument limits the growth of the tree).\n",
    "    - In the 20 questions analogy, this is like \"How many questions we can ask?\"\n",
    "    \n",
    "    \n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "    - By default, the minimum number of samples required to split is 2. That is, if there are two or more observations in a node and if we haven't already achieved maximum purity, we can split it!\n",
    "    \n",
    "    \n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node (a terminal node at the end of the tree).\n",
    "    - By default, the minimum number of samples required in a leaf node is 1. (This should ring alarm bells - it's very possible that we'll overfit our model to the data!)\n",
    "\n",
    "\n",
    "- `ccp_alpha`: A [complexity parameter](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning) similar to $\\alpha$ in regularization. As `ccp_alpha` increases, we regularize more.\n",
    "    - By default, this value is 0.\n",
    "\n",
    "[Source: Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate model with:\n",
    "# # - a maximum depth of 5.\n",
    "# # - at least 7 samples required in order to split an internal node.\n",
    "# # - at least 3 samples in each leaf node.\n",
    "# # - a cost complexity of 0.01.\n",
    "# # - random state of 42.\n",
    "\n",
    "# dt = DecisionTreeClassifier(max_depth = 5,\n",
    "#                             min_samples_split = 7,\n",
    "#                             min_samples_leaf = 3,\n",
    "#                             ccp_alpha = 0.01,\n",
    "#                             random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5a878",
   "metadata": {},
   "source": [
    "## Use GridSearch to tune hyperparameters and find better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e192b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b088028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(estimator = DecisionTreeClassifier(),\n",
    "#                     param_grid = {'max_depth': [2, 3, 5, 7],\n",
    "#                                   'min_samples_split': [5, 10, 15, 20],\n",
    "#                                   'min_samples_leaf': [2, 3, 4, 5, 6],\n",
    "#                                   'ccp_alpha': [0, 0.001, 0.01, 0.1, 1, 10]},\n",
    "#                     cv = 5,\n",
    "#                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b825a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # Start our timer.\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Let's GridSearch over the above parameters on our training data.\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # Stop our timer and print the result.\n",
    "# print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa042e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What is our best decision tree?\n",
    "# grid.best_estimator_\n",
    "\n",
    "# # What was the cross-validated score of the above decision tree?\n",
    "# grid.best_score_\n",
    "\n",
    "# # Evaluate model.\n",
    "# print(f'Score on training set: {grid.score(X_train, y_train)}')\n",
    "# print(f'Score on testing set: {grid.score(X_test, y_test)}')\n",
    "\n",
    "# # Generate predictions on test set.\n",
    "# preds = grid.predict(X_test)\n",
    "\n",
    "# # Import confusion_matrix.\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # Generate confusion matrix.\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test,\n",
    "#                                   preds).ravel()\n",
    "\n",
    "# print(confusion_matrix(y_test, preds))\n",
    "\n",
    "# # Calculate sensitivity.\n",
    "\n",
    "# sens = tp / (tp + fn)\n",
    "# print(f'Sensitivity: {round(sens, 4)}')\n",
    "\n",
    "# # Calculate specificity.\n",
    "\n",
    "# spec = tn / (tn + fp)\n",
    "# print(f'Specificity: {round(spec, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f92b96",
   "metadata": {},
   "source": [
    "### Why use a decision tree?\n",
    "\n",
    "1. We don't have to scale our data\n",
    "2. Decision trees don't make assumptions about how our data is distributed\n",
    "3. Easy to interpret (feature importance)\n",
    "4. Speed (fit very quickly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ca599",
   "metadata": {},
   "source": [
    "### Why not use a decision tree?\n",
    "\n",
    "1. Decision trees can very easily overfit.\n",
    "2. Decision trees are locally optimal: Because we're making the best decision at each node (greedy), we might end up with a worse solution in the long run.\n",
    "3. Decision trees don't work well with unbalanced data. (Check out the `class_weight` parameter if you're interested.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ed8a0",
   "metadata": {},
   "source": [
    "## Decision trees vs logistic regression\n",
    "\n",
    "- **Interpretability**: The coefficients in a logistic regression model are interpretable. (They represent the change in log-odds caused by the input variables.) However, this is complicated and not easy for non-technical audiences. Decision trees are interpretable; it is easy to explain to show a picture of a decision tree to a client or boss and get them to understand how predictions are made.\n",
    "<br>\n",
    "\n",
    "- **Performance**: Decision trees have a tendency to easily overfit, while logistic regression models usually do not overfit as easily.\n",
    "<br>\n",
    "\n",
    "- **Assumptions**: Decision trees have no assumptions about how data are distributed; logistic regression does make assumptions about how data are distributed.\n",
    "<br>\n",
    "\n",
    "- **Frequency**: Logistic regression is more commonly than decision trees.\n",
    "<br>\n",
    "\n",
    "- **Y variable**: Decision trees can handle regression and classification problems; logistic regression is only really used for classification problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
