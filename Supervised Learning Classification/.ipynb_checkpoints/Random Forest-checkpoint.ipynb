{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f00da80",
   "metadata": {},
   "source": [
    "## Random Forests vs Bagged trees\n",
    "---\n",
    "\n",
    "With bagged decision trees, we generate many different trees on pretty similar data. These trees are **strongly correlated** with one another. Because these trees are correlated with one another, they will have high variance.\n",
    "\n",
    "By \"de-correlating\" our trees from one another, we can drastically reduce the variance of our model.\n",
    "\n",
    "That's the difference between bagged decision trees and random forests! This will reduce our variance (at the expense of a small increase in bias) and thus should greatly improve the overall performance of the final model.\n",
    "\n",
    "### So how do we \"de-correlate\" our trees?\n",
    "\n",
    "Random forests differ from bagging decision trees in only one way: They use a modified tree learning algorithm that selects, at each split in the learning process, a **random subset of the features**. This process is sometimes called the *random subspace method*.\n",
    "\n",
    "TLDR: Only use a few features, not all like bagged trees.\n",
    "\n",
    "For a problem with $p$ features, it is typical to use:\n",
    "\n",
    "- $\\sqrt{p}$ (rounded down) features in each split for a classification problem.\n",
    "- $p/3$ (rounded down) with a minimum node size of 5 as the default for a regression problem.\n",
    "\n",
    "Random forests, a step beyond bagged decision trees, are **very widely used** classifiers and regressors. They are relatively simple to use because they require very few parameters to set and they perform pretty well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d73e6",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees (ExtraTrees)\n",
    "Adding another step of randomization (and thus de-correlation) yields extremely randomized trees, or _ExtraTrees_. Like Random Forests, these are trained using the random subspace method (sampling of features). However, they are trained on the entire dataset instead of bootstrapped samples. A layer of randomness is introduced in the way the nodes are split. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity) for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range.\n",
    "\n",
    "This further reduces the variance, but causes an increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebfe05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "# from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b9ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate\n",
    "# rf = RandomForestClassifier(n_estimators=100)\n",
    "# et = ExtraTreesClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca21cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cross val score\n",
    "# cross_val_score(rf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84ba467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gridsearch over hyperparameters\n",
    "\n",
    "# rf_params = {\n",
    "#     'n_estimators': [100, 150, 200],\n",
    "#     'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(rf, param_grid=rf_params, cv=5)\n",
    "# gs.fit(X_train, y_train)\n",
    "# print(gs.best_score_)\n",
    "# gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
