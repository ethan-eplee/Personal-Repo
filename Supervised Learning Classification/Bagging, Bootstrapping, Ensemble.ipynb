{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8703756",
   "metadata": {},
   "source": [
    "## Where do decision trees tend to fall on the Bias/Variance spectrum?</summary>\n",
    "    \n",
    "- Decision trees very easily overfit.\n",
    "- They tend to suffer from **high error due to variance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df0199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba7ce513",
   "metadata": {},
   "source": [
    "## Bootstrapping is **random resampling with replacement**.\n",
    "\n",
    "The idea is this:\n",
    "- Take your original sample of data, with sample size $n$.\n",
    "- Take many sub-samples (say $B$) of size $n$ from your sample **with replacement**. These are called **bootstrapped samples**.\n",
    "- You have now generated $B$ bootstrapped samples, where each sample is of size $n$!\n",
    "<br>\n",
    "\n",
    "- Instead of building one model on our original sample, we will now build one model on each bootstrapped sample, giving us $B$ models in total!\n",
    "- Experience tells us that combining the models from our bootstrapped samples will be closer to what we'd see from the population than to just get one model from our original sample.\n",
    "\n",
    "This sets up the idea of an **ensemble model**.\n",
    "\n",
    "- Bootstrapping is random resampling with replacement.\n",
    "- We bootstrap when fitting bagged decision trees so that we can fit multiple decision trees on slightly different sets of data. **Bagged decision trees tend to outperform single decision trees.**\n",
    "- Bootstrapping can also be used to conduct hypothesis tests and generate confidence intervals directly from resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec519012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39324005",
   "metadata": {},
   "source": [
    "## Introduction to Ensemble Methods\n",
    "\n",
    "Different types of models we've built thus far:\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- $k$-Nearest Neighbors\n",
    "- Naive Bayes Classification\n",
    "\n",
    "Same type of process:\n",
    "1. Based on our problem, we identify which model to use. (Is our problem classification or regression? Do we want an interpretable model?)\n",
    "2. Fit the model using the training data.\n",
    "3. Use the fit model to generate predictions.\n",
    "4. Evaluate our model's performance and, if necessary, return to step 2 and make changes.\n",
    "\n",
    "So far, we've always had **exactly one model**. Today, however, we're going to talk about **ensemble methods**. Mentally, you should think about this as if we build multiple models and then aggregate their results in some way.\n",
    "\n",
    "## Why would we build an \"ensemble model?\"\n",
    "\n",
    "Our goal is to estimate $f$, the true function. (Think about $f$ as the **true process** that dictates Ames housing prices.)\n",
    "\n",
    "We can come up with different models $m_1$, $m_2$, and so on to get as close to $f$ as possible. (Think about $m_1$ as the model you built to predict $f$, think of $m_2$ as the model your neighbor built to predict $f$, and so on.)\n",
    "\n",
    "## Three Benefits: Statistical, Computational, Representational\n",
    "- The **statistical** benefit to ensemble methods: By building one model, our predictions are almost certainly going to be wrong. Predictions from one model might overestimate housing prices; predictions from another model might underestimate housing prices. By \"averaging\" predictions from multiple models, we'll see that we can often cancel our errors out and get closer to the true function $f$.\n",
    "<br>\n",
    "\n",
    "- The **computational** benefit to ensemble methods: It might be impossible to develop one model that globally optimizes our objective function. (Remember that CART reach locally-optimal solutions that aren't guaranteed to be the globally-optimal solution.) In these cases, it may be **impossible** for one CART to arrive at the true function $f$. However, generating many different models and averaging their predictions may allow us to get results that are closer to the global optimum than any individual model.\n",
    "<br>\n",
    "\n",
    "- The **representational** benefit to ensemble methods: Even if we had all the data and all the computer power in the world, it might be impossible for one model to **exactly** equal $f$. For example, a linear regression model can never model a relationship where a one-unit change in $X$ is associated with some *different* change in $Y$ based on the value of $X$. All models have some shortcomings. (See [the no free lunch theorems](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization).) While individual models have shortcomings, by creating multiple models and aggregating their predictions, we can actually create predictions that represent something that one model cannot ever represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb103b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5daafba",
   "metadata": {},
   "source": [
    "## Bagging: Bootstrap Aggregating\n",
    "\n",
    "Decision trees are powerful machine learning models. However, decision trees have some limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns (a.k.a. they overfit their training sets). \n",
    "\n",
    "Bagging (bootstrap aggregating) mitigates this problem by exposing different trees to different sub-samples of the training set.\n",
    "\n",
    "The process for creating bagged decision trees is as follows:\n",
    "1. From the original data of size $n$, bootstrap $B$ samples each of size $n$ (with replacement!).\n",
    "2. Build a decision tree on each bootstrapped sample.\n",
    "3. Make predictions by passing a test observation through all $B$ trees and developing one aggregate prediction for that observation.\n",
    "\n",
    "## \"Aggregate prediction?\"\n",
    "As with all of our modeling techniques, we want to make sure that we can come up with one final prediction for each observation.\n",
    "\n",
    "Suppose we want to predict whether or not a Reddit post is going to go viral, where `1` indicates viral and `0` indicates non-viral. We build 100 decision trees. Given a new Reddit post labeled `X_test`, we pass these features into all 100 decision trees.\n",
    "- 70 of the trees predict that the post in `X_test` will go viral.\n",
    "- 30 of the trees predict that the post in `X_test` will not go viral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f1398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb097",
   "metadata": {},
   "source": [
    "What might you expect .predict(X_test) to output?\n",
    "\n",
    "- `.predict(X_test)` should output a 1, predicting that the post will go viral.\n",
    "\n",
    "What might you expect .predict_proba(X_test) to output?\n",
    "\n",
    "- `.predict_proba(X_test)` should output [0.3 0.7], indicating the probability of the post going viral is 70% and the probability of the post not going viral to be 30%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb9f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de7a027a",
   "metadata": {},
   "source": [
    "## Hard code a bagging classifier using for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1b6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate dataframe.\n",
    "# predictions = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "# # Generate ten decision trees.\n",
    "# for i in range(1, 11):\n",
    "    \n",
    "#     # Bootstrap X data.\n",
    "#     # Should we add a random seed?\n",
    "#     X_sample = X_train.sample(n = X_train.shape[0],\n",
    "#                               replace=True)\n",
    "    \n",
    "#     # Get y data that matches the X data.\n",
    "#     y_sample = y_train[X_sample.index]\n",
    "    \n",
    "#     # Instantiate decision tree.\n",
    "#     t = DecisionTreeClassifier()\n",
    "    \n",
    "#     # Fit to our sample data.\n",
    "#     t.fit(X_sample, y_sample)\n",
    "    \n",
    "#     # Put predictions in dataframe.\n",
    "#     predictions[f'Tree {i}'] = t.predict(X_test)\n",
    "\n",
    "# # Generate aggregated predicted probabilities.\n",
    "# probs = predictions.mean(axis='columns')\n",
    "\n",
    "# accuracy_score(y_test, (probs > .5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a48dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b72f140",
   "metadata": {},
   "source": [
    "## Using sklearn for bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2666860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate BaggingClassifier.\n",
    "# bag = BaggingClassifier(random_state = 42)\n",
    "\n",
    "# # Fit BaggingClassifier.\n",
    "# bag.fit(X_train, y_train)\n",
    "\n",
    "# # Score BaggingClassifier.\n",
    "# bag.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b0d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
